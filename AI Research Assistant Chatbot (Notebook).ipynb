{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e6dbff",
   "metadata": {},
   "source": [
    "### Build an Advanced AI Research Assistant (Chatbot) that integrates LLM, RAG, and Agent-based architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86ab41",
   "metadata": {},
   "source": [
    "##### Import Useful Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67855c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import faiss\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323622e5",
   "metadata": {},
   "source": [
    "##### File Path and Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5391078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = os.path.join(os.getcwd(), \"s2orc Dataset\", \"ComputerScience,2022-2022_test.jsonl\")\n",
    "DEVICE = \"cpu\"  # ðŸ”’ For CPU only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5969497",
   "metadata": {},
   "source": [
    "##### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "382010d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 326 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>247411061</td>\n",
       "      <td>DUNE Software and High Performance Computing</td>\n",
       "      <td>DUNE, like other HEP experiments, faces a chal...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[Computer Science]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>247597192</td>\n",
       "      <td>A Perspective on Neural Capacity Estimation: V...</td>\n",
       "      <td>Recently, several methods have been proposed f...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[Computer Science]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>245668726</td>\n",
       "      <td>Adaptive Template Enhancement for Improved Per...</td>\n",
       "      <td>A novel instance-based method for the classifi...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[Computer Science]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>248300244</td>\n",
       "      <td>Re-Examining System-Level Correlations of Auto...</td>\n",
       "      <td>How reliably an automatic summarization evalua...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[Computer Science]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>254854674</td>\n",
       "      <td>Iso-Dream: Isolating and Leveraging Noncontrol...</td>\n",
       "      <td>World models learn the consequences of actions...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[Computer Science]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0  247411061       DUNE Software and High Performance Computing   \n",
       "1  247597192  A Perspective on Neural Capacity Estimation: V...   \n",
       "2  245668726  Adaptive Template Enhancement for Improved Per...   \n",
       "3  248300244  Re-Examining System-Level Correlations of Auto...   \n",
       "4  254854674  Iso-Dream: Isolating and Leveraging Noncontrol...   \n",
       "\n",
       "                                             content  year               field  \n",
       "0  DUNE, like other HEP experiments, faces a chal...  2022  [Computer Science]  \n",
       "1  Recently, several methods have been proposed f...  2022  [Computer Science]  \n",
       "2  A novel instance-based method for the classifi...  2022  [Computer Science]  \n",
       "3  How reliably an automatic summarization evalua...  2022  [Computer Science]  \n",
       "4  World models learn the consequences of actions...  2022  [Computer Science]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_cleaned_papers(input_file, max_docs, target_field):\n",
    "    documents = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        count = 0\n",
    "        for line in infile:\n",
    "            paper = json.loads(line)\n",
    "            if paper.get(\"metadata\", {}).get(\"s2fieldsofstudy\", [None])[0] != target_field:\n",
    "                continue\n",
    "            raw_text = paper.get(\"text\", \"\")\n",
    "            title_match = re.match(r\"^(.*?)\\n\", raw_text.strip())\n",
    "            title = title_match.group(1).strip() if title_match else \"Untitled\"\n",
    "            content = raw_text.replace(title, \"\").strip()\n",
    "\n",
    "            documents.append({\n",
    "                \"id\": paper[\"id\"],\n",
    "                \"title\": title,\n",
    "                \"content\": content,\n",
    "                \"year\": paper[\"metadata\"].get(\"year\", None),\n",
    "                \"field\": paper[\"metadata\"].get(\"s2fieldsofstudy\", [])\n",
    "            })\n",
    "\n",
    "            count += 1\n",
    "            if count >= max_docs:\n",
    "                break\n",
    "    return documents\n",
    "\n",
    "documents = extract_cleaned_papers(INPUT_FILE, max_docs=5000, target_field=\"Computer Science\")\n",
    "print(f\"âœ… Loaded {len(documents)} documents\")\n",
    "df = pd.DataFrame(documents)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55383d",
   "metadata": {},
   "source": [
    "##### Chunking: Chunk documents into passages\n",
    "Chunks the document into overlapping chunks using a sliding window over sentences. This avoids splitting mid-sentence and keeps semantic coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee73e8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Chunking documents...\n",
      "Total passages after chunking: 752\n",
      "\n",
      "ðŸ“„ Chunk 1 (Title: DUNE Software and High Performance Computing):\n",
      "DUNE, like other HEP experiments, faces a challenge related to matching execution patterns of our production simulation and data processing software to the limitations imposed by modern high-performance computing facilities. In order to efficiently exploit these new architectures, particularly those with high CPU core counts and GPU accelerators, our existing software execution models require adaptation. In addition, the large size of individual units of raw data from the far detector modules pose an additional challenge somewhat unique to DUNE. Here we describe some of these problems and how we begin to solve them today with existing software frameworks and toolkits. We also describe ways we may leverage these existing software architectures to attack remaining problems going forward. This whitepaper is a contribution to the Computational Frontier of Snowmass21. Introduction\n",
      "DUNE [1] software faces many software development and data processing challenges shared with other large HEP experiments. Moore's law has morphed from delivering ever faster CPUs to delivering ever more numerous and sometimes even slower cores. This is epitomized by GPUs which provide O(1k) cores per card but which are clocked far slower than even low-end CPUs. We must develop software that can efficiently exploit the highly parallel architectures of high-performance computers (HPC) while still making efficient use of the more familiar high-throughput computing (HTC) facilities that offer modest parallelism of O(10) CPU cores per compute node. Contemporaneously, the available RAM capacity associated with CPU and GPU is not growing as fast as their core counts and RAM/core ratio has become a limiting parameter for many types of jobs. Where possible we must reimplement and rearchitect to reduce memory waste. On the other hand, many classes of jobs have an unavoidable memory overhead.\n",
      "\n",
      "ðŸ”¢ Tokens: 282\n",
      "\n",
      "ðŸ“„ Chunk 2 (Title: DUNE Software and High Performance Computing):\n",
      "It works by allowing only a limited number of GPU tasks to be submitted at any given time. Once the limit is reached, a subsequent task must wait for \"its turn\". Currently, this waiting is implemented by allowing the thread and thus the CPU core that issued the task to go idle. This idleness will tend to strike and remove cores from making progress precisely when CPU tasks are most needed. To break this bottleneck we plan to implement a so far ignored type of node provided by the TBB flow_graph library called async_node. Such a node is allowed a free thread under the assumption it will be lightly used. This allows an asynchronous query protocol with the graph execution engine. Essentially, the node may be periodically checked for completion. This can cause delay in the GPU task return but does not lead to CPU nor GPU idleness. Another imperfection with the current semaphore based GPU load balancing is that the semaphore is implemented at the C++ language level. As such it produces a tight code binding between all flow-graph node implementations that wish to issue GPU tasks. This goes against the general wellfactored, interface-based code architecture of the toolkit. As the semaphore is process-local it is useless for moderation of GPU access which is shared across processes. To solve this we expect to develop a distributed application composed of multiple processes based on art or WCT or in some cases both. The flow-graphs will have nodes that provide communication between processes which potentially may be executing on different compute nodes. Initial plans for this distributed architecture were based on the ZIO [9] project. More recently, investigation into ADIOS2 [10] show it as a promising and likely preferred alternative.\n",
      "\n",
      "ðŸ”¢ Tokens: 289\n"
     ]
    }
   ],
   "source": [
    "def chunk_document(doc, max_tokens=300, stride=150):\n",
    "    sentences = sent_tokenize(doc)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_len = 0\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        current_chunk = []\n",
    "        current_len = 0\n",
    "        j = i\n",
    "        while j < len(sentences) and current_len + len(sentences[j].split()) <= max_tokens:\n",
    "            current_chunk.append(sentences[j])\n",
    "            current_len += len(sentences[j].split())\n",
    "            j += 1\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        i += stride if stride > 0 else j  # Slide forward by 'stride' or jump to next non-overlapping\n",
    "    return chunks\n",
    "\n",
    "print(\"ðŸ”„ Chunking documents...\")\n",
    "chunked_docs = []\n",
    "titles = []\n",
    "for doc in documents:\n",
    "    chunks = chunk_document(doc[\"content\"], max_tokens=300, stride=150)\n",
    "    chunked_docs.extend(chunks)\n",
    "    titles.extend([doc[\"title\"]] * len(chunks))\n",
    "print(f\"Total passages after chunking: {len(chunked_docs)}\")\n",
    "\n",
    "# Print sample chunks with the title\n",
    "for i in range(2):\n",
    "    print(f\"\\nðŸ“„ Chunk {i+1} (Title: {titles[i]}):\")\n",
    "    print(chunked_docs[i])\n",
    "    print(f\"\\nðŸ”¢ Tokens: {len(chunked_docs[i].split())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736f650a",
   "metadata": {},
   "source": [
    "##### Prepare dataset for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0e918cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Modeling and Estimation of CO2 Emissions in China Based on Artificial Intelligence',\n",
       "  'text': \"Since China's reform and opening up, the social economy has achieved rapid development, followed by a sharp increase in carbon dioxide (CO2) emissions. Therefore, at the 75th United Nations General Assembly, China proposed to achieve carbon peaking by 2030 and carbon neutrality by 2060. The research work on advance forecasting of CO2 emissions is essential to achieve the above-mentioned carbon peaking and carbon neutrality goals in China. In order to achieve accurate prediction of CO2 emissions, this study establishes a hybrid intelligent algorithm model suitable for CO2 emissions prediction based on China's CO2 emissions and related socioeconomic indicator data from 1971 to 2017. The hyperparameters of Least Squares Support Vector Regression (LSSVR) are optimized by the Adaptive Artificial Bee Colony (AABC) algorithm to build a high-performance hybrid intelligence model. The research results show that the hybrid intelligent algorithm model designed in this paper has stronger robustness and accuracy with relative error almost within Â±5% in the advance prediction of CO2 emissions. The modeling scheme proposed in this study can not only provide strong support for the Chinese government and industry departments to formulate policies related to the carbon peaking and carbon neutrality goals, but also can be extended to the research of other socioeconomic-related issues. Introduction\\nGlobal warming has become a fact generally accepted by the international community. Climate warming has seriously affected the living environment and social development of humankind. Although the cyclical changes in the natural environment itself affect global climate change, more and more studies have shown that human activities have accelerated the global warming process largely. Since the industrial revolution, with the development of social economy and the increasing intensity of human activities, the concentration of carbon dioxide (CO 2 ) in the atmosphere has risen sharply [1].\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"title\": titles, \"text\": chunked_docs}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "ef = dataset.to_pandas()\n",
    "ef.insert(0, \"chunk_id\", range(1, len(ef)+1))\n",
    "\n",
    "random.sample([dict(i) for i in dataset], k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ade21",
   "metadata": {},
   "source": [
    "##### Embedding Models\n",
    "For our project, we go with:\n",
    "<br>1) **multiqa_mini:** Multi-domain QA MiniLM model\n",
    "<br>2) **scibert:** SciBERT fine-tuned on scientific vocab\n",
    "<br>1) **allmini_v2:** General-purpose MiniLM v2 embeddings\n",
    "<br>2) **mpnet_base:** High-accuracy general-purpose MPNet model\n",
    "\n",
    "Each document is encoded into a high-dimensional vector using SentenceTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dab454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Processing with model: multiqa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6050f5245a4d4e7fa7de14f6da6398d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Processing with model: scibert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7422933c4a34df9973fa7c28a165ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Processing with model: allmini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9802b1b53a20442e887eab4baadff53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Processing with model: mpnet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944f0441d30b4295b4b13381cc373b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>embedding_multiqa</th>\n",
       "      <th>embedding_scibert</th>\n",
       "      <th>embedding_allmini</th>\n",
       "      <th>embedding_mpnet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>DUNE Software and High Performance Computing</td>\n",
       "      <td>DUNE, like other HEP experiments, faces a chal...</td>\n",
       "      <td>[0.07001133263111115, -0.09193330258131027, -0...</td>\n",
       "      <td>[0.17008523643016815, -0.41661253571510315, 0....</td>\n",
       "      <td>[-0.19705259799957275, 0.0813499465584755, -0....</td>\n",
       "      <td>[0.035186517983675, 0.04750188812613487, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>DUNE Software and High Performance Computing</td>\n",
       "      <td>It works by allowing only a limited number of ...</td>\n",
       "      <td>[-0.11874090135097504, -0.09827910363674164, 0...</td>\n",
       "      <td>[0.038926806300878525, -0.09388677775859833, -...</td>\n",
       "      <td>[-0.1930292844772339, 0.05765102058649063, -0....</td>\n",
       "      <td>[-0.024255581200122833, -0.10105226188898087, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A Perspective on Neural Capacity Estimation: V...</td>\n",
       "      <td>Recently, several methods have been proposed f...</td>\n",
       "      <td>[-0.060939595103263855, -0.14694444835186005, ...</td>\n",
       "      <td>[0.5142149925231934, -0.32449233531951904, -0....</td>\n",
       "      <td>[-0.06463883817195892, -0.13580818474292755, -...</td>\n",
       "      <td>[-0.07576467096805573, 0.08724159747362137, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A Perspective on Neural Capacity Estimation: V...</td>\n",
       "      <td>In estimating MI, the samples for the product ...</td>\n",
       "      <td>[-0.08345496654510498, -0.16691753268241882, -...</td>\n",
       "      <td>[0.31848376989364624, -0.11525746434926987, -0...</td>\n",
       "      <td>[-0.1146080270409584, -0.10533967614173889, 0....</td>\n",
       "      <td>[-0.09548977017402649, 0.054860033094882965, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>A Perspective on Neural Capacity Estimation: V...</td>\n",
       "      <td>When comparing the numerical results with the ...</td>\n",
       "      <td>[0.01412767730653286, -0.14586158096790314, 0....</td>\n",
       "      <td>[0.16452282667160034, -0.18880560994148254, -0...</td>\n",
       "      <td>[-0.037659913301467896, -0.08678082376718521, ...</td>\n",
       "      <td>[-0.12054596096277237, -0.013146807439625263, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_id                                              title  \\\n",
       "0         1       DUNE Software and High Performance Computing   \n",
       "1         2       DUNE Software and High Performance Computing   \n",
       "2         3  A Perspective on Neural Capacity Estimation: V...   \n",
       "3         4  A Perspective on Neural Capacity Estimation: V...   \n",
       "4         5  A Perspective on Neural Capacity Estimation: V...   \n",
       "\n",
       "                                                text  \\\n",
       "0  DUNE, like other HEP experiments, faces a chal...   \n",
       "1  It works by allowing only a limited number of ...   \n",
       "2  Recently, several methods have been proposed f...   \n",
       "3  In estimating MI, the samples for the product ...   \n",
       "4  When comparing the numerical results with the ...   \n",
       "\n",
       "                                   embedding_multiqa  \\\n",
       "0  [0.07001133263111115, -0.09193330258131027, -0...   \n",
       "1  [-0.11874090135097504, -0.09827910363674164, 0...   \n",
       "2  [-0.060939595103263855, -0.14694444835186005, ...   \n",
       "3  [-0.08345496654510498, -0.16691753268241882, -...   \n",
       "4  [0.01412767730653286, -0.14586158096790314, 0....   \n",
       "\n",
       "                                   embedding_scibert  \\\n",
       "0  [0.17008523643016815, -0.41661253571510315, 0....   \n",
       "1  [0.038926806300878525, -0.09388677775859833, -...   \n",
       "2  [0.5142149925231934, -0.32449233531951904, -0....   \n",
       "3  [0.31848376989364624, -0.11525746434926987, -0...   \n",
       "4  [0.16452282667160034, -0.18880560994148254, -0...   \n",
       "\n",
       "                                   embedding_allmini  \\\n",
       "0  [-0.19705259799957275, 0.0813499465584755, -0....   \n",
       "1  [-0.1930292844772339, 0.05765102058649063, -0....   \n",
       "2  [-0.06463883817195892, -0.13580818474292755, -...   \n",
       "3  [-0.1146080270409584, -0.10533967614173889, 0....   \n",
       "4  [-0.037659913301467896, -0.08678082376718521, ...   \n",
       "\n",
       "                                     embedding_mpnet  \n",
       "0  [0.035186517983675, 0.04750188812613487, -0.03...  \n",
       "1  [-0.024255581200122833, -0.10105226188898087, ...  \n",
       "2  [-0.07576467096805573, 0.08724159747362137, 0....  \n",
       "3  [-0.09548977017402649, 0.054860033094882965, -...  \n",
       "4  [-0.12054596096277237, -0.013146807439625263, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embed_chunk(model_name, dataset, batch_size=32):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    text_chunks = [chunk[\"text\"] for chunk in dataset]\n",
    "\n",
    "    if \"scibert\" in model_name:      # Preprocess and truncate input for scibert model\n",
    "        final_chunks = []\n",
    "        for text in text_chunks:\n",
    "            input_ids = tokenizer.encode(text, max_length=512, truncation=True, add_special_tokens=True)\n",
    "            decoded_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "            final_chunks.append(decoded_text)\n",
    "    else:\n",
    "        final_chunks = text_chunks\n",
    "\n",
    "    embeddings_tensor = model.encode(final_chunks, batch_size=batch_size, convert_to_tensor=True, show_progress_bar=True)\n",
    "    embeddings_list = embeddings_tensor.cpu().numpy().tolist()\n",
    "    return [embeddings_tensor, embeddings_list]\n",
    "\n",
    "\n",
    "embedding_models = {\n",
    "    \"multiqa\": \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\",\n",
    "    \"scibert\": \"allenai/scibert_scivocab_uncased\",\n",
    "    \"allmini\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"mpnet\": \"sentence-transformers/all-mpnet-base-v2\"\n",
    "}\n",
    "\n",
    "model_embed_list = []\n",
    "for key, name in embedding_models.items():\n",
    "    print(f\"\\nðŸ”„ Processing with model: {key}\")\n",
    "    e = embed_chunk(name, dataset)\n",
    "    model_embed_list.append(e[0])\n",
    "    ef[\"embedding_\"+key] = e[1]\n",
    "\n",
    "# ef.to_excel('Embeddings.xlsx', index=False)\n",
    "ef.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b609a091",
   "metadata": {},
   "source": [
    "##### FAISS index\n",
    "The embeddings are stored in a FAISS index for efficient similarity search. The FAISS index allows us to retrieve the most semantically similar documents to a given query in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8209878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Doc Index: 0, Similarity Score: 1.0000\n",
      "2. Doc Index: 293, Similarity Score: 0.9455\n",
      "3. Doc Index: 336, Similarity Score: 0.9325\n"
     ]
    }
   ],
   "source": [
    "tensors_multiqa = model_embed_list[0]\n",
    "tensors_scibert = model_embed_list[1]\n",
    "tensors_allmini = model_embed_list[2]\n",
    "tensors_mpnet   = model_embed_list[3]\n",
    "\n",
    "# Convert PyTorch tensors to NumPy\n",
    "np_multiqa = tensors_multiqa.detach().cpu().numpy().astype('float32')\n",
    "np_scibert = tensors_scibert.detach().cpu().numpy().astype('float32')\n",
    "np_allmini = tensors_allmini.detach().cpu().numpy().astype('float32')\n",
    "np_mpnet   = tensors_mpnet.detach().cpu().numpy().astype('float32')\n",
    "\n",
    "# Normalize embeddings and create a cosine similarity FAISS index\n",
    "def build_faiss_cosine_index(embeddings_np):\n",
    "    faiss.normalize_L2(embeddings_np)\n",
    "    dim = embeddings_np.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(embeddings_np)\n",
    "    return index\n",
    "\n",
    "# Create indexes\n",
    "faiss_indexes = {\n",
    "    \"multiqa\": build_faiss_cosine_index(np_multiqa),\n",
    "    \"scibert\": build_faiss_cosine_index(np_scibert),\n",
    "    \"allmini\": build_faiss_cosine_index(np_allmini),\n",
    "    \"mpnet\":   build_faiss_cosine_index(np_mpnet),\n",
    "}\n",
    "\n",
    "# To find top-k similar documents to a query embedding\n",
    "def search_faiss(index, query_tensor, top_k=3):\n",
    "    query_np = query_tensor.detach().cpu().numpy().astype('float32').reshape(1, -1)\n",
    "    faiss.normalize_L2(query_np)\n",
    "    distances, indices = index.search(query_np, top_k)\n",
    "    return distances[0], indices[0]\n",
    "\n",
    "# Example: search similar documents using a Scibert embedding\n",
    "query_tensor = tensors_scibert[0] \n",
    "dists, idxs = search_faiss(faiss_indexes[\"scibert\"], query_tensor, top_k=3)\n",
    "for rank, (i, score) in enumerate(zip(idxs, dists), 1):\n",
    "    print(f\"{rank}. Doc Index: {i}, Similarity Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4629bde",
   "metadata": {},
   "source": [
    "##### Implement a Retrieval Function\n",
    "A function that takes a user query, embeds it using the same model, and retrieves the most relevant documents from the FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c93f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Rank</th>\n",
       "      <th>FAISS Score</th>\n",
       "      <th>Reranker Score</th>\n",
       "      <th>Doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multiqa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.532506</td>\n",
       "      <td>0.702883</td>\n",
       "      <td>Several researchers have employed deep learnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scibert</td>\n",
       "      <td>1</td>\n",
       "      <td>0.640150</td>\n",
       "      <td>-11.057257</td>\n",
       "      <td>We sat in the outdoor patio area next to a few...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allmini</td>\n",
       "      <td>1</td>\n",
       "      <td>0.562497</td>\n",
       "      <td>0.702883</td>\n",
       "      <td>Several researchers have employed deep learnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mpnet</td>\n",
       "      <td>1</td>\n",
       "      <td>0.540243</td>\n",
       "      <td>0.702883</td>\n",
       "      <td>Several researchers have employed deep learnin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model  Rank  FAISS Score  Reranker Score  \\\n",
       "0  multiqa     1     0.532506        0.702883   \n",
       "1  scibert     1     0.640150      -11.057257   \n",
       "2  allmini     1     0.562497        0.702883   \n",
       "3    mpnet     1     0.540243        0.702883   \n",
       "\n",
       "                                                 Doc  \n",
       "0  Several researchers have employed deep learnin...  \n",
       "1  We sat in the outdoor patio area next to a few...  \n",
       "2  Several researchers have employed deep learnin...  \n",
       "3  Several researchers have employed deep learnin...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieve_top_docs_with_scores_and_rerank(query_text, dataset, faiss_indexes, top_k=3):\n",
    "    embedding_models = {\n",
    "        \"multiqa\": \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\",\n",
    "        \"scibert\": \"allenai/scibert_scivocab_uncased\",\n",
    "        \"allmini\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"mpnet\": \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    }\n",
    "    reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
    "    \n",
    "    results = {}\n",
    "    for model_name, model_path in embedding_models.items():\n",
    "        model = SentenceTransformer(model_path)\n",
    "        query_embedding = model.encode(query_text, convert_to_tensor=True)\n",
    "        distances, indices = search_faiss(faiss_indexes[model_name], query_embedding, top_k)\n",
    "        \n",
    "        # Get top docs with FAISS scores\n",
    "        top_docs = [(dataset[int(i)][\"text\"], float(distances[idx])) for idx, i in enumerate(indices)]\n",
    "        \n",
    "        # Get top docs with scores after reranking\n",
    "        rerank_inputs = [(query_text, doc_text) for doc_text, _ in top_docs]\n",
    "        rerank_scores = reranker.predict(rerank_inputs)\n",
    "        combined = list(zip(top_docs, rerank_scores))  # [ ((doc_text, faiss_score), rerank_score), ... ]\n",
    "        combined_sorted = sorted(combined, key=lambda x: x[1], reverse=True)   # Sort by reranker score descending\n",
    "        top_docs_with_scores = [(doc_score[0], doc_score[1], rerank_score) for (doc_score, rerank_score) in combined_sorted]\n",
    "        results[model_name] = top_docs_with_scores\n",
    "        \n",
    "        # Get results in tabular format\n",
    "        rows = []\n",
    "        for model_name, docs in results.items():\n",
    "            for rank, (doc_text, faiss_score, rerank_score) in enumerate(docs, 1):\n",
    "                rows.append({\n",
    "                    \"Model\": model_name,\n",
    "                    \"Rank\": rank,\n",
    "                    \"FAISS Score\": faiss_score,\n",
    "                    \"Reranker Score\": rerank_score,\n",
    "                    \"Doc\": doc_text\n",
    "                })\n",
    "        df_results = pd.DataFrame(rows)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "# Example:\n",
    "query = \"Can you tell me something about deep learning?\"\n",
    "results = retrieve_top_docs_with_scores_and_rerank(query, dataset, faiss_indexes, top_k=1)\n",
    "results.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05a84b2",
   "metadata": {},
   "source": [
    "##### Integrate a Language Model (LLM)\n",
    "Generate a response using the LLM with retrieved document context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aa886a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¬ Start chatting with your AI assistant (type 'exit' to stop)...\n",
      "\n",
      "ðŸ§‘ You: What challenge do modern high-performance computing (HPC) facilities pose for HEP experiments like DUNE?\n",
      "ðŸ“„ Top doc snippet: DUNE, like other HEP experiments, faces a challenge related to matching execution patterns of our pr...\n",
      "ðŸ¤– AI: matching execution patterns of our production simulation and data processing software to the limitations imposed by modern high-performance computing facilities\n",
      "\n",
      "ðŸ§‘ You: Why does the large size of individual raw data units from the far detector modules pose a challenge for DUNE?\n",
      "ðŸ“„ Top doc snippet: DUNE, like other HEP experiments, faces a challenge related to matching execution patterns of our pr...\n",
      "ðŸ¤– AI: DUNE, like other HEP experiments, faces a challenge related to matching execution patterns of our production simulation and data processing software to the limitations imposed by modern high-performance computing facilities\n",
      "\n",
      "ðŸ§‘ You: How has Mooreâ€™s law changed in recent years, and how does this impact computing architectures?\n",
      "ðŸ“„ Top doc snippet: DUNE, like other HEP experiments, faces a challenge related to matching execution patterns of our pr...\n",
      "ðŸ¤– AI: Moore's law has morphed from delivering ever faster CPUs to delivering ever more numerous and sometimes even slower cores\n",
      "\n",
      "ðŸ§‘ You: How does the current GPU task submission system work?\n",
      "ðŸ“„ Top doc snippet: It works by allowing only a limited number of GPU tasks to be submitted at any given time. Once the ...\n",
      "ðŸ¤– AI: allowing only a limited number of GPU tasks to be submitted at any given time\n",
      "\n",
      "ðŸ§‘ You: What is the main drawback of the current waiting mechanism for GPU tasks?\n",
      "ðŸ“„ Top doc snippet: It works by allowing only a limited number of GPU tasks to be submitted at any given time. Once the ...\n",
      "ðŸ¤– AI: idleness will tend to strike and remove cores from making progress precisely when CPU tasks are most needed\n",
      "\n",
      "ðŸ§‘ You: What solution is proposed to break the bottleneck caused by CPU idleness?\n",
      "ðŸ“„ Top doc snippet: It works by allowing only a limited number of GPU tasks to be submitted at any given time. Once the ...\n",
      "ðŸ¤– AI: I'm not sure. Can you rephrase or ask a different question?\n",
      "\n",
      "ðŸ§‘ You: How does the use of an async_node improve GPU task handling?\n",
      "ðŸ“„ Top doc snippet: It works by allowing only a limited number of GPU tasks to be submitted at any given time. Once the ...\n",
      "ðŸ¤– AI: This can cause delay in the GPU task return but does not lead to CPU nor GPU idleness\n",
      "\n",
      "ðŸ§‘ You: What approach is proposed to handle GPU access across multiple processes?\n",
      "ðŸ“„ Top doc snippet: It works by allowing only a limited number of GPU tasks to be submitted at any given time. Once the ...\n",
      "ðŸ¤– AI: a distributed application composed of multiple processes based on art or WCT or in some cases both\n",
      "\n",
      "ðŸ§‘ You: exit\n",
      "ðŸ‘‹ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Load LLM model and tokenizer\n",
    "llm_model_name = \"google/flan-t5-large\"\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "llm_model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n",
    "\n",
    "# Answer generation\n",
    "def generate_answer(query, context_docs, model, tokenizer, device, max_gen_length=150):\n",
    "    context = context_docs if isinstance(context_docs, str) else \"\\n---\\n\".join(context_docs)\n",
    "    prompt = f\"Answer the question based on the following documents.\\n\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=min(512, tokenizer.model_max_length))\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            max_length=min(1024, inputs[\"input_ids\"].shape[1] + max_gen_length),\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=4,\n",
    "            do_sample=False\n",
    "        )\n",
    "    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "    if len(answer.split()) < 3 or any(term in answer for term in [\"BVI\", \"Q-Task\", \"BTC\", \"coding gain\"]):\n",
    "        return \"I'm not sure. Can you rephrase or ask a different question?\"\n",
    "    return answer\n",
    "\n",
    "# CLI Chat Loop\n",
    "def chat_loop(dataset, faiss_indexes, llm_model, llm_tokenizer, DEVICE):\n",
    "    print(\"\\nðŸ’¬ Start chatting with your AI assistant (type 'exit' to stop)...\\n\")\n",
    "    while True:\n",
    "        query = input(\"ðŸ§‘ You: \").strip()\n",
    "        if query.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"ðŸ‘‹ Goodbye!\")\n",
    "            break\n",
    "        df_results = retrieve_top_docs_with_scores_and_rerank(query, dataset, faiss_indexes, top_k=1)\n",
    "        best_doc_text = df_results.loc[df_results[\"Reranker Score\"].idxmax()][\"Doc\"]\n",
    "        if best_doc_text:\n",
    "            print(\"ðŸ“„ Top doc snippet:\", best_doc_text[:100].replace('\\n', ' ') + \"...\")\n",
    "        else:\n",
    "            print(\"ðŸ“„ No relevant documents found.\\n\")\n",
    "        answer = generate_answer(query, best_doc_text, llm_model, llm_tokenizer, DEVICE)\n",
    "        print(f\"ðŸ¤– AI: {answer}\\n\")\n",
    "        \n",
    "chat_loop(dataset, faiss_indexes, llm_model, llm_tokenizer, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
